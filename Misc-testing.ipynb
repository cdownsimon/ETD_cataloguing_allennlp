{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
    "from allennlp.nn import util\n",
    "\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = LinearMatrixAttention(800, 800, 'x,y,x*y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 69])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.rand(1,1,800)\n",
    "y = th.rand(1,69,800)\n",
    "self_attention(x,y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = th.randn(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2188, -0.6998,  1.1127,  0.3072,  1.0043],\n",
       "        [ 0.6953,  0.4697, -0.0422,  0.3413, -1.2633],\n",
       "        [ 1.5753,  0.4542,  1.2470,  1.1029, -0.4689]])"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = th.tensor([[1,1,1,0,0],[0,0,0,1,0],[1,0,1,0,0]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2188, -0.6998,  1.1127,  0.3072,  1.0043],\n",
       "         [ 0.6953,  0.4697, -0.0422,  0.3413, -1.2633],\n",
       "         [ 1.5753,  0.4542,  1.2470,  1.1029, -0.4689]]),\n",
       " tensor([[ 1.,  1.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions,gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6998, -1.2633, -0.4689])"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "top_k = predictions.topk(k)[0][:,k-1]\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.ge(predictions,top_k.unsqueeze(1).expand(predictions.size(0),predictions.size(1))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((((gold_labels + predictions) >= 2).sum(1).float()/gold_labels.sum(1).float()).sum()/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.rand(26,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 756.,  764.,  265.,  260.,  380.,  516.,  389.,  331.,  366.,\n",
       "         317.,  595.,  628.,  428.,  796.,    6.,  347.,  729.,  207.,\n",
       "          61.,  255.,  614.,  244.,  109.,  711.,  317.,  320.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max(-1)[1].float().squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = th.tensor([[1,0,0],[1,0,1],[0,1,1],[0,1,0]]).float()\n",
    "predict = th.tensor([[1,0,0],[0,0,1],[1,0,0],[0,0,1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(true*predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(true*predict)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  1.,  0.,  0.]), tensor([ 1.,  0.,  1.,  0.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:,0],predict[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = true[:,0]\n",
    "p0 = predict[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t0*p0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1-t0)*(1-p0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-(t0*p0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p0-t0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1-t0)*p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t0*(1-p0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  1.,  1.],\n",
       "        [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((true*predict).sum(1)/true.sum(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  1.,  1.],\n",
       "        [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:,l] * predict[:,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=0\n",
    "(true[:,l] * predict[:,l]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(true*predict).sum(0)[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=2\n",
    "((1-true[:,l]) * (1- predict[:,l])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1-true)*(1-predict)).sum(0)[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "_true_positives = (true*predict).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "_false_positives = (true*predict).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = _true_positives / (_false_positives + 1e-13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2. * ((tmp * tmp) / (tmp + tmp + 1e-13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = FeedForward(5,2,10,'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for n, param in linear.named_parameters():\n",
    "    if 'weight' in n:\n",
    "        params.append(param)\n",
    "target_param = params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1179, -0.2996,  0.1698,  0.2341,  0.1861,  0.2195, -0.0739,  0.1006,\n",
       "         -0.1433,  0.1459],\n",
       "        [ 0.1558, -0.1414, -0.0286, -0.3127, -0.2545,  0.0639,  0.2007, -0.0902,\n",
       "         -0.0379,  0.0910],\n",
       "        [-0.1573,  0.2527,  0.0598, -0.0828, -0.2393, -0.2237,  0.1925, -0.0457,\n",
       "         -0.2069, -0.2214],\n",
       "        [ 0.1934,  0.2431,  0.1339,  0.0987, -0.2624,  0.3061, -0.0556, -0.2501,\n",
       "          0.1696, -0.1801],\n",
       "        [ 0.1682,  0.0968,  0.0831,  0.2584,  0.1386, -0.0639,  0.2007,  0.0686,\n",
       "         -0.1568, -0.2099],\n",
       "        [-0.0444,  0.0677,  0.1154,  0.1922,  0.3126, -0.2419, -0.1068,  0.2502,\n",
       "          0.1152,  0.1067],\n",
       "        [ 0.2451, -0.1460, -0.2428,  0.1853, -0.2830,  0.0117,  0.1820,  0.1565,\n",
       "          0.2472,  0.3086],\n",
       "        [ 0.2736, -0.2389,  0.2741,  0.1414,  0.0016,  0.3063, -0.2257,  0.2314,\n",
       "          0.1119,  0.0261],\n",
       "        [ 0.2017, -0.2562, -0.1686,  0.2812,  0.2450, -0.1068, -0.2439,  0.0668,\n",
       "         -0.0094, -0.1221],\n",
       "        [ 0.3132,  0.0466,  0.1576, -0.1501, -0.2074, -0.1885, -0.1106,  0.2202,\n",
       "         -0.1880, -0.3103]], requires_grad=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1558, -0.1414, -0.0286, -0.3127, -0.2545,  0.0639,  0.2007, -0.0902,\n",
       "         -0.0379,  0.0910],\n",
       "        [ 0.1558, -0.1414, -0.0286, -0.3127, -0.2545,  0.0639,  0.2007, -0.0902,\n",
       "         -0.0379,  0.0910],\n",
       "        [ 0.1558, -0.1414, -0.0286, -0.3127, -0.2545,  0.0639,  0.2007, -0.0902,\n",
       "         -0.0379,  0.0910],\n",
       "        [ 0.1558, -0.1414, -0.0286, -0.3127, -0.2545,  0.0639,  0.2007, -0.0902,\n",
       "         -0.0379,  0.0910],\n",
       "        [ 0.1934,  0.2431,  0.1339,  0.0987, -0.2624,  0.3061, -0.0556, -0.2501,\n",
       "          0.1696, -0.1801],\n",
       "        [-0.0444,  0.0677,  0.1154,  0.1922,  0.3126, -0.2419, -0.1068,  0.2502,\n",
       "          0.1152,  0.1067]], grad_fn=<TakeBackward>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_param[[1,1,1,1,3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8165, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.norm(target_param[[1,3,5]]-target_param[[5,7,1]],2,1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = th.optim.SGD(linear.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0126,  0.2827, -0.4353,  0.0226,  0.0056],\n",
       "        [-0.0643,  0.3521, -0.2954, -0.3986, -0.0568],\n",
       "        [-0.2083, -0.2609,  0.0186, -0.1921,  0.1349],\n",
       "        [ 0.3564,  0.4360,  0.0539,  0.0931,  0.1806],\n",
       "        [ 0.2896,  0.4071,  0.2728,  0.0194,  0.2296],\n",
       "        [-0.1640,  0.2470,  0.3195, -0.4202, -0.0181],\n",
       "        [ 0.0951, -0.3350, -0.3095,  0.1322, -0.1912],\n",
       "        [ 0.2780,  0.0138, -0.1370,  0.4208, -0.0639],\n",
       "        [ 0.1533, -0.3238, -0.2623,  0.3911, -0.1782],\n",
       "        [ 0.4145, -0.2262,  0.3164, -0.2626,  0.0993]], requires_grad=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4545, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.norm(params[0][1][0]-params[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = th.nn.LSTM(10,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1670,  0.2917,  0.2211,  0.0153,  0.4960, -0.2319,  0.2642, -0.9624,\n",
       "           0.3835, -0.8068],\n",
       "         [-0.7509, -0.2114, -0.7882, -0.5555,  0.8680, -0.0634,  0.4944,  0.8323,\n",
       "           0.0534, -0.2827],\n",
       "         [ 0.9128, -0.2959,  0.7704,  0.2507, -0.2608,  0.6896,  0.7365, -0.7078,\n",
       "          -0.2602,  0.5349],\n",
       "         [-0.9607,  0.5065,  0.4523, -0.2891, -0.9092,  0.7098,  0.9130, -0.1830,\n",
       "           0.6092,  0.7942]], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.5741],\n",
       "         [ 0.1430],\n",
       "         [ 0.6417],\n",
       "         [ 0.7707]], requires_grad=True), Parameter containing:\n",
       " tensor([0.9047, 0.3792, 0.2942, 0.3699], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.4149, -0.7786, -0.0173,  0.3669], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.8633],\n",
       "         [ 0.8483],\n",
       "         [-0.2649],\n",
       "         [ 0.4731]], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.1533],\n",
       "         [ 0.9004],\n",
       "         [ 0.6605],\n",
       "         [ 0.1624]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.6038, -0.6861,  0.4161, -0.0979], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.9451, -0.6624, -0.8092, -0.5897], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.4281],\n",
       "         [ 0.5353],\n",
       "         [-0.8739],\n",
       "         [ 0.2798]], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.7095],\n",
       "         [-0.0504],\n",
       "         [-0.1716],\n",
       "         [-0.5098]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.9872,  0.1007,  0.1633, -0.1517], requires_grad=True), Parameter containing:\n",
       " tensor([-0.1431,  0.1228, -0.0881, -0.2916], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.3091],\n",
       "         [-0.5204],\n",
       "         [ 0.2758],\n",
       "         [-0.4490]], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2117],\n",
       "         [-0.9534],\n",
       "         [ 0.1476],\n",
       "         [ 0.8099]], requires_grad=True), Parameter containing:\n",
       " tensor([0.9898, 0.5595, 0.4743, 0.2504], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0499,  0.8529, -0.8394, -0.4879], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.9925],\n",
       "         [ 0.9087],\n",
       "         [-0.4523],\n",
       "         [ 0.2990]], requires_grad=True), Parameter containing:\n",
       " tensor([[0.8729],\n",
       "         [0.7450],\n",
       "         [0.0191],\n",
       "         [0.6497]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.9407,  0.1918, -0.6173,  0.2344], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.2763,  0.2712, -0.3831,  0.2040], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.6890],\n",
       "         [ 0.7382],\n",
       "         [-0.9457],\n",
       "         [-0.3514]], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.3318],\n",
       "         [-0.8303],\n",
       "         [-0.4432],\n",
       "         [-0.9718]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.6102,  0.7750,  0.2182, -0.3848], requires_grad=True), Parameter containing:\n",
       " tensor([-0.4272, -0.0380,  0.5343,  0.2763], requires_grad=True), Parameter containing:\n",
       " tensor([[0.4516],\n",
       "         [0.1860],\n",
       "         [0.4935],\n",
       "         [0.9205]], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.6025],\n",
       "         [ 0.6377],\n",
       "         [ 0.7437],\n",
       "         [-0.1945]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.8217,  0.1456, -0.2555,  0.7294], requires_grad=True), Parameter containing:\n",
       " tensor([0.0206, 0.3300, 0.2305, 0.7336], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.8197],\n",
       "         [-0.4646],\n",
       "         [-0.5581],\n",
       "         [-0.8749]], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.7240],\n",
       "         [ 0.1113],\n",
       "         [ 0.4801],\n",
       "         [-0.5581]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.7561, -0.3834, -0.4094,  0.4624], requires_grad=True), Parameter containing:\n",
       " tensor([-0.8844, -0.8634,  0.4935,  0.8264], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.3257],\n",
       "         [-0.2858],\n",
       "         [ 0.5069],\n",
       "         [ 0.5825]], requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.2044],\n",
       "         [ 0.0705],\n",
       "         [-0.1345],\n",
       "         [-0.6384]], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.2559,  0.3554,  0.6024, -0.7722], requires_grad=True), Parameter containing:\n",
       " tensor([-0.6448,  0.2717, -0.2480, -0.5027], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2337],\n",
       "         [ 0.0733],\n",
       "         [ 0.1845],\n",
       "         [ 0.2956]], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2962],\n",
       "         [-0.5117],\n",
       "         [ 0.6019],\n",
       "         [-0.2401]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.5826, -0.5904, -0.0422, -0.3116], requires_grad=True), Parameter containing:\n",
       " tensor([ 0.2663, -0.7987, -0.5123, -0.6681], requires_grad=True)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lstm.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for l,n in enumerate(hl):\n",
    "    y = th.zeros(n,5*len(hl))\n",
    "    y[:,l*5:(l+1)*5] = 1\n",
    "    t.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0:5,3:6]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.cat(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2884, -0.6896,  1.4009, -0.0840],\n",
       "         [ 1.2884, -0.6896,  1.4009, -0.0840],\n",
       "         [ 1.2884, -0.6896,  1.4009, -0.0840],\n",
       "         [ 1.2884, -0.6896,  1.4009, -0.0840],\n",
       "         [ 1.2884, -0.6896,  1.4009, -0.0840]],\n",
       "\n",
       "        [[ 0.5904,  0.6352, -1.7919, -0.9090],\n",
       "         [ 0.5904,  0.6352, -1.7919, -0.9090],\n",
       "         [ 0.5904,  0.6352, -1.7919, -0.9090],\n",
       "         [ 0.5904,  0.6352, -1.7919, -0.9090],\n",
       "         [ 0.5904,  0.6352, -1.7919, -0.9090]],\n",
       "\n",
       "        [[-0.5692,  2.1354,  0.0221, -0.0875],\n",
       "         [-0.5692,  2.1354,  0.0221, -0.0875],\n",
       "         [-0.5692,  2.1354,  0.0221, -0.0875],\n",
       "         [-0.5692,  2.1354,  0.0221, -0.0875],\n",
       "         [-0.5692,  2.1354,  0.0221, -0.0875]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(1).expand(-1,5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from my_library.losses.hierarchical_graph import HierarchicalGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lcsh/sh_path.json','r') as d:\n",
    "    subject_paths = {}\n",
    "    lines = d.readlines()\n",
    "    for l in lines:\n",
    "        subject_paths.update(json.loads(l.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/lcsh/sh_hierarchy.json', \"r\") as hierarchy_file:\n",
    "    etd_hierarchy = json.load(hierarchy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/all_etd_title_abstract_train_new.json', \"r\") as data_file:\n",
    "    etd_jsons = []\n",
    "    for line in data_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "        etd_json = json.loads(line)\n",
    "        abstract = etd_json['etdAbstract']\n",
    "        labels = etd_json['lcsh']\n",
    "\n",
    "        etd_jsons.append((abstract,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47446"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcsh = set(itertools.chain(*[l.keys() for _,l in etd_jsons]))\n",
    "len(lcsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_level = {}\n",
    "for l,sh in etd_hierarchy.items():\n",
    "    intersect = list(lcsh.intersection(sh))\n",
    "    if intersect:\n",
    "        groupby_level[int(l)-1] = intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "covered = list(itertools.chain(*[j for _,j in groupby_level.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered = (lcsh.difference(covered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47446"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = {}\n",
    "idx = 0\n",
    "for l,sh in groupby_level.items():\n",
    "    for i in sh:\n",
    "        class_index[i] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../class_index.json','w') as f:\n",
    "    json.dump(class_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/index/all_etd_title_abstract_class_index.json','r') as f:\n",
    "    class_to_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2c = {i:c for c,i in c.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Science', 'Cognitive science', 'Artificial intelligence', 'Machine learning']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_paths['Machine learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(etd_jsons[5][1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [subject_paths[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ability',\n",
       " 'Art',\n",
       " 'Arts',\n",
       " 'Criminals',\n",
       " 'Culture',\n",
       " 'Human face recognition (Computer science)',\n",
       " 'Humanities',\n",
       " 'Identification photographs',\n",
       " 'Intellectual life',\n",
       " 'Learning ability',\n",
       " 'Learning and scholarship',\n",
       " 'Optical pattern recognition',\n",
       " 'Perceptrons',\n",
       " 'Persons',\n",
       " 'Photographs',\n",
       " 'Pictures',\n",
       " 'Self-organizing systems'}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(itertools.chain(*paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,ls in etd_jsons:\n",
    "    labels = list(ls.keys())\n",
    "    paths = [subject_paths[l] for l in labels]\n",
    "    assert [class_to_idx[l] for l in set(itertools.chain(*paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [list(l.keys()) for _,l in etd_jsons]\n",
    "all_labels = set(itertools.chain(*all_labels))\n",
    "all_labels = set(itertools.chain(*[subject_paths[l] for l in all_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hand-to-hand fighting, Oriental']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_paths['Hand-to-hand fighting, Oriental']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in mads_subjects_graph._roots:\n",
    "    subject_paths.update({r:[r]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53879"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_level = {}\n",
    "for l,sh in etd_hierarchy.items():\n",
    "    intersect = list(all_labels.intersection(sh))\n",
    "    if intersect:\n",
    "        groupby_level[int(l)-1] = intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {}\n",
    "idx = 0\n",
    "for l,sh in groupby_level.items():\n",
    "    for i in sh:\n",
    "        class_to_idx[i] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,ls in etd_jsons:\n",
    "    labels = list(ls.keys())\n",
    "    paths = [subject_paths[l] for l in labels]\n",
    "    assert [class_to_idx[l] for l in set(itertools.chain(*paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53879"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(itertools.chain(*[j for _,j in groupby_level.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10255"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx['Hand-to-hand fighting, Oriental']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../test.json','w') as f:\n",
    "    json.dump(groupby_level, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../test.json','r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11102, 10222, 8014, 8058, 6240, 4298, 2539, 1479, 972, 572, 245, 129, 9]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(l) for _,l in test.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/lcsh/parsed/authoritiessubjects.dict'\n",
    "with open(file_path,'rb') as d:\n",
    "    mads_subjects = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417721/417721 [00:01<00:00, 304994.56it/s]\n"
     ]
    }
   ],
   "source": [
    "missed = []\n",
    "missed_lcsh = []\n",
    "contained =[]\n",
    "connections = []\n",
    "for sb, details in tqdm(mads_subjects.items()):\n",
    "    narrowers = details['narrowers']\n",
    "    if narrowers:\n",
    "        cnt = len(narrowers)\n",
    "        for nar in narrowers:\n",
    "            if nar not in mads_subjects:\n",
    "                missed.append(nar)\n",
    "                cnt -= 1\n",
    "                continue\n",
    "            connections.append((details['authoritativeLabel'],mads_subjects[nar]['authoritativeLabel']))\n",
    "            contained.append(details['authoritativeLabel'])\n",
    "            contained.append(mads_subjects[nar]['authoritativeLabel'])\n",
    "        if cnt == 0:\n",
    "            missed_lcsh.append(details['authoritativeLabel'])\n",
    "    else:\n",
    "        missed_lcsh.append(details['authoritativeLabel'])\n",
    "root_n_leaf  = list(set(missed_lcsh).difference(contained))\n",
    "connections = {'link': connections, 'root_n_leaf': root_n_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads_subjects_graph = HierarchicalGraph(directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63027/63027 [00:00<00:00, 1056965.21it/s]\n"
     ]
    }
   ],
   "source": [
    "mads_subjects_graph.load(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233727"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_nodes = list(mads_subjects_graph._nodes.keys())\n",
    "len(graph_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417721/417721 [00:00<00:00, 1859521.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "417721"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mads_subjects_authoritativeLabel = [details['authoritativeLabel'] for sb, details in tqdm(mads_subjects.items())]\n",
    "len(mads_subjects_authoritativeLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncovered.intersection(graph_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncovered.difference(mads_subjects_authoritativeLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncovered.difference(missed_lcsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncovered.difference(contained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncovered.difference(root_n_leaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Cold war' in mads_subjects_authoritativeLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Cold War' in mads_subjects_authoritativeLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i.lower() for i in uncovered]).difference([i.lower() for i in missed_lcsh+contained])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([i.lower() for i in uncovered]).difference([i.lower() for i in mads_subjects_authoritativeLabel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416469"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(contained+missed_lcsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416469"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mads_subjects_authoritativeLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mads_subjects_authoritativeLabel).difference(list(mads_subjects_graph._nodes.keys())+missed_lcsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(mads_subjects_authoritativeLabel).difference(list(mads_subjects_graph._nodes.keys())+missed_lcsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233727"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(contained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353776"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(missed_lcsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(contained+missed_lcsh).difference(mads_subjects_authoritativeLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mads_subjects_authoritativeLabel).difference(contained+missed_lcsh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233727"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mads_subjects_graph._nodes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mads_subjects_graph._nodes.keys()).intersection(set(root_n_leaf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182742"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(missed_lcsh).difference(mads_subjects_graph._nodes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "\"In order to skip indexing, your labels must be integers. Found labels = ['Child development']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-f607f8d81556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMultiLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_indexing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/misc/projdata3/info_fil/finance/simon_test/allennlp_official/allennlp/allennlp/data/fields/multilabel_field.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, labels, label_namespace, skip_indexing, num_labels)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 raise ConfigurationError(\"In order to skip indexing, your labels must be integers. \"\n\u001b[0;32m---> 66\u001b[0;31m                                          \"Found labels = {}\".format(labels))\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In order to skip indexing, num_labels can't be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: \"In order to skip indexing, your labels must be integers. Found labels = ['Child development']\""
     ]
    }
   ],
   "source": [
    "MultiLabelField([label for label,value in labels.items() if value == 1],skip_indexing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_sentences.unsqueeze(1).expand(-1, 13, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = encoded_sentences.unsqueeze(0) if len(encoded_sentences.size()) < 2 else encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8127, -1.5018,  0.1780, -0.6542,  1.0974,  1.0849,  0.2327,  1.0172,\n",
       "          1.1240, -0.6912]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8533,  0.4190, -2.0767, -0.0345,  0.6630, -0.7116,  1.5221,\n",
       "           0.7992, -1.0465, -0.3173],\n",
       "         [-0.6149,  0.9646,  0.3574,  1.1369,  2.1833,  0.9439, -2.7625,\n",
       "           1.0525, -0.0339, -1.8345],\n",
       "         [ 0.8095, -0.5504, -0.2730, -0.9327,  0.8884, -1.0047,  0.0709,\n",
       "          -1.1145, -0.6126, -1.0040],\n",
       "         [-0.4275,  1.4740,  1.6313, -0.3532, -0.5005, -0.0942, -1.6845,\n",
       "           0.5843, -0.1975,  0.4605],\n",
       "         [ 0.5792, -1.4113, -0.3771, -0.4636, -0.2917, -1.4306,  0.9793,\n",
       "          -0.7659, -0.8269, -0.3076],\n",
       "         [ 0.8639, -1.4813, -1.0534, -0.0316, -0.4078, -1.0978,  0.1940,\n",
       "          -0.1055, -0.2257,  0.8761],\n",
       "         [-0.4007, -0.1054, -0.4576,  0.0587, -0.5834, -2.8705,  1.7095,\n",
       "          -2.5253, -0.5600, -1.8780],\n",
       "         [-0.0916,  0.4525, -0.4344,  0.7702, -0.3336,  0.0061,  0.4410,\n",
       "           0.5020,  1.9787,  0.8349],\n",
       "         [ 0.8058, -2.2734, -0.5170,  0.4205,  0.6850, -1.3153,  2.1278,\n",
       "           0.0507, -0.5713, -0.4724],\n",
       "         [-2.0789, -0.4287,  0.8910,  0.0215, -1.1284,  0.9739, -0.5581,\n",
       "           1.1910, -1.4022,  1.7863],\n",
       "         [-0.0517, -1.7080,  0.7533, -0.5152, -0.1819,  1.1709,  0.2803,\n",
       "           0.9416,  1.0650, -0.4134],\n",
       "         [-2.2963, -0.2260,  0.5261,  1.2114, -1.2216,  0.8044,  0.9269,\n",
       "          -0.2096, -0.9021, -0.0166],\n",
       "         [ 0.5139, -0.0946,  0.4919, -0.6781,  0.7307, -0.5236, -0.5699,\n",
       "           1.2727, -0.6638,  1.0263]],\n",
       "\n",
       "        [[-0.6869,  1.0328,  0.1451,  0.9924, -0.2513,  1.4701, -0.1578,\n",
       "          -1.2405, -0.4850, -1.4975],\n",
       "         [-0.0721, -0.8242, -1.0884,  0.3376,  1.2036,  0.9692,  0.4073,\n",
       "           0.9092, -0.0283, -0.7537],\n",
       "         [-0.5078, -1.1650, -0.4861, -1.4129,  0.5540,  2.0595,  0.8215,\n",
       "          -0.9009, -1.9122, -0.3688],\n",
       "         [-0.0155, -0.2553,  0.4005, -0.6113,  1.6710,  0.0337, -0.8708,\n",
       "          -2.4953, -1.8854,  0.9633],\n",
       "         [-0.7123, -1.8385, -2.1088, -0.2985,  0.0459,  0.9134, -0.1783,\n",
       "          -0.9327,  0.7999,  1.8288],\n",
       "         [ 2.1960,  0.8943,  0.7237, -0.9055, -0.6696,  0.5195,  0.1323,\n",
       "           0.1343, -1.3420,  0.0365],\n",
       "         [ 0.0833, -0.2976,  0.9501, -1.5266, -0.5259,  0.0504,  0.8282,\n",
       "          -0.1346,  0.9025,  1.4702],\n",
       "         [ 0.9396,  0.9046, -1.6221,  0.1365,  0.9351, -0.0046,  0.2397,\n",
       "           0.3324, -0.3390,  0.8564],\n",
       "         [ 0.5401, -0.0226, -0.3994,  0.4823,  1.1543, -0.4708,  1.9201,\n",
       "           0.7003, -0.2672,  0.4469],\n",
       "         [-0.6190,  1.5758,  0.4051, -0.9814,  0.8685,  0.4269, -1.1819,\n",
       "          -1.1218,  1.8805,  1.5152],\n",
       "         [ 1.1708, -0.4336, -0.3338, -0.2846,  0.5621, -0.3539,  1.5070,\n",
       "          -0.5618,  0.7741,  0.5419],\n",
       "         [ 0.7613,  1.1445, -0.3508,  0.8674,  0.0415, -0.0514,  3.3765,\n",
       "           0.2646, -1.5777,  0.2960],\n",
       "         [ 2.2604, -0.0663, -0.0501,  0.8773,  0.4897, -0.2203,  1.2558,\n",
       "           0.0123, -0.0590,  0.7235]]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,13,10)\n",
    "w = [torch.randn(i+1,10) for i in range(13)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6059, -6.7942,  0.8046,  3.8361,  3.9413,  4.3066,  2.4927,  3.5499,\n",
       "          1.0587, -2.4760, -0.7733, -0.7845,  0.0579,  2.7780,  2.3800,  1.3649,\n",
       "         -1.5505,  3.3124, -0.1399,  5.7586, -1.6194,  2.9416, -1.8218,  6.2885,\n",
       "          0.7569, -2.7944,  5.4584, -9.3708,  3.1077,  5.0431,  0.0940, -4.0578,\n",
       "         -0.8638,  4.5924, -0.4655, -1.6509, -1.4189, -0.0906, -0.7262,  5.1204,\n",
       "          0.3162, -1.1249,  2.6144,  1.9246,  1.2589,  4.3415,  9.1092,  3.2742,\n",
       "          4.7613,  3.6194,  0.9449,  0.4045,  4.8451, -2.9840, -2.1515,  2.1143,\n",
       "         -2.6395, -0.2799,  0.3988, -1.3586,  2.8291,  0.3997,  2.3667,  5.7298,\n",
       "         -4.0207,  2.0423,  2.1108, -3.7583, -3.3623, -3.4064,  5.1331, -0.3764,\n",
       "         -3.4477,  6.2547, -1.7320,  2.1923, -5.0698,  0.6963,  2.2288,  3.2331,\n",
       "         -1.6346,  3.9578,  1.2025, -1.2038, -0.2823, -0.4072,  2.7594, -0.1961,\n",
       "         -2.2852,  2.0799,  3.4738],\n",
       "        [-0.3312,  0.1029,  2.8425,  5.0372,  7.8872, -1.5711,  4.1248, -1.0108,\n",
       "         -1.3150,  1.2805, -1.2835, -0.0230,  3.6639,  5.6173,  3.5873, -2.3458,\n",
       "          4.3535, -0.6270,  0.3595, -1.4621, -2.0927,  1.7139,  0.1843,  0.4564,\n",
       "          1.7498, -2.8020,  2.0698,  3.5446,  5.4008,  4.3256,  0.5736, -1.6036,\n",
       "          3.1898,  2.0587,  0.1730, -0.5904, -0.2831,  0.3936, -1.2221,  1.0840,\n",
       "         -2.3418, -0.9780,  1.7586,  4.8693,  2.8976,  2.6944, -1.6269, -3.2969,\n",
       "         -1.7726,  6.2159, -0.3996, -2.1557,  2.6792,  3.0002, -2.5420,  1.8990,\n",
       "          1.0065,  1.2010, -0.4190,  0.9425,  2.1295, -0.6882, -0.3103,  3.5292,\n",
       "         -0.3107,  3.8166, -4.7771, -0.3322,  4.4106,  1.6340,  1.0068, -1.6778,\n",
       "          6.1062,  1.7377, -3.1243,  5.1107, -1.8463,  1.2513, -1.0384, -4.1292,\n",
       "         -2.4315,  8.1022,  0.8879,  3.6873,  0.3572,  3.0908, -5.3799, -3.3032,\n",
       "          3.7503,  1.4242,  0.4460]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.nn.functional.linear(x[:,i,:],w[i],None) for i in range(13)], dim=1) + torch.ones(91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7855, 0.4102, 0.1455, 0.6103, 0.5276],\n",
       "        [0.1901, 0.0534, 0.2029, 0.7552, 0.1263],\n",
       "        [0.0534, 0.8271, 0.0519, 0.6432, 0.1110]])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5600)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[:,[1,3,4,4,4,4,4,4]] - x[:,[3,3,3,3,3,3,3,3]]).pow(2).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2), (3, 4)}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(1,2),(1,2),(3,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-373-fa523135fe44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "[3,2,1][-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_parent_pairs = []\n",
    "with open('./data/index/etd_debug_child_parents.csv','r') as f:\n",
    "    for l in f.readlines():\n",
    "        pair = l.strip().split(',')\n",
    "        child_parent_pairs.append((int(pair[0]),int(pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9586,  0.7083,  1.1294,  0.8688, -0.9867,  0.2612,  0.3218,  1.2775,\n",
       "          0.3404, -0.5256],\n",
       "        [ 0.1447, -0.0454,  2.1842, -0.8901, -0.6541,  0.7833, -0.8478,  0.6289,\n",
       "          1.2716,  1.1189],\n",
       "        [ 0.9744,  0.6664,  0.4691,  0.6247, -1.2273, -1.2953,  0.1572,  0.2920,\n",
       "          0.0888, -0.7402]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 1., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 1., 1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (torch.rand(3,10) > 0.5).float()\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7140,  1.5113,  1.2477],\n",
       "         [ 0.9857,  0.5312,  0.2728],\n",
       "         [ 1.6699,  1.1535, -0.0951]]), tensor([[0, 2, 4],\n",
       "         [7, 6, 3],\n",
       "         [1, 5, 3]]))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.topk(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0]])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([[1,1,0]])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x.unsqueeze(0),x.unsqueeze(0)],dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
    "from allennlp.nn import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = LinearMatrixAttention(10,10,'x,y,x*y,x/y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(torch.cat([x.unsqueeze(0),x.unsqueeze(0)],dim=0),torch.cat([x.unsqueeze(0),x.unsqueeze(0)],dim=0)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6fcf9dfbd479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders.multi_head_self_attention import MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhattn = MultiHeadSelfAttention(1,10,10,10,1)\n",
    "mhattn(x.unsqueeze(0)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = util.masked_softmax(mhattn(x.unsqueeze(0)).transpose(1,2),mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.5000, 0.0000]]], grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33814086"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3338*1.7140+0.3338*-0.0093+0.3325*-0.6944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9586,  0.7083,  1.1294,  0.8688, -0.9867,  0.2612,  0.3218,  1.2775,\n",
       "          0.3404, -0.5256],\n",
       "        [ 0.1447, -0.0454,  2.1842, -0.8901, -0.6541,  0.7833, -0.8478,  0.6289,\n",
       "          1.2716,  1.1189],\n",
       "        [ 0.9744,  0.6664,  0.4691,  0.6247, -1.2273, -1.2953,  0.1572,  0.2920,\n",
       "          0.0888, -0.7402]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3381,  0.5187, -0.3685, -0.6713,  0.0726,  0.2058, -0.0107,\n",
       "           0.2366, -0.4881, -0.7352]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.weighted_sum(x.unsqueeze(0), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/projdata3/info_fil/finance/simon_test/allennlp_official/allennlp/allennlp/service/predictors/__init__.py:23: FutureWarning: allennlp.service.predictors.* has been depreciated. Please use allennlp.predictors.*\n",
      "  \"Please use allennlp.predictors.*\", FutureWarning)\n",
      "/misc/projdata3/info_fil/finance/simon_test/allennlp_official/allennlp/allennlp/service/predictors/predictor.py:6: FutureWarning: allennlp.service.predictors.* has been deprecated. Please use allennlp.predictors.*\n",
      "  \" Please use allennlp.predictors.*\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from my_library.models.etd_attention import MultiHeadAttentionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiHeadAttentionEncoder(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3,10)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1,1,0],[0,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3324, -0.7735,  2.0834,  0.3789, -1.7387, -0.2029,  0.7632, -0.5713,\n",
       "         -0.0396, -1.0012],\n",
       "        [ 1.1416, -1.1636, -0.2683,  0.4592, -0.8949,  0.2587, -0.0739,  0.0943,\n",
       "          0.4202,  0.0145]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.ones(5),torch.ones(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAN Allennlp Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict, Optional, Union\n",
    "from typing import List, Dict, Iterable\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from overrides import overrides\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import ListField, TextField, SequenceLabelField, MultiLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder, Seq2SeqEncoder\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator, util\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "from my_library.metrics.roc_auc_score import RocAucScore\n",
    "from my_library.metrics.hit_at_k import *\n",
    "from my_library.metrics.macro_f1 import *\n",
    "from my_library.metrics.precision_at_k import *\n",
    "from my_library.models.etd_attention import AttentionEncoder, MultiHeadAttentionEncoder\n",
    "from my_library.models.customized_allennlp.maxout import Maxout\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "global x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtdHANAbstractReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a CSV-lines file containing abstract only from ETD records\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 merge_title_abstract: bool = False,\n",
    "                 lazy: bool = False,\n",
    "                 start_tokens: List[str] = [\"<start>\"], \n",
    "                 end_tokens: List[str] = [\"<end>\"]) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer(start_tokens=start_tokens,end_tokens=end_tokens)\n",
    "        self._sentence_splitter = spacy.load('en_core_web_sm')\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self._merge_title_abstract = merge_title_abstract\n",
    "        \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "#             logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "            for line in data_file:\n",
    "                line = line.strip(\"\\n\")\n",
    "                if not line:\n",
    "                    continue\n",
    "                etd_json = json.loads(line)\n",
    "                labels = etd_json['lcsh']\n",
    "                if self._merge_title_abstract and 'etdTitle' in  etd_json:\n",
    "                    abstract = '%s @@@SEP@@@ %s'%(etd_json['etdTitle'],etd_json['etdAbstract'])\n",
    "                else:\n",
    "                    abstract = etd_json['etdAbstract']\n",
    "                labels = etd_json['lcsh']\n",
    "\n",
    "                yield self.text_to_instance(abstract, labels)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, abstract_text: str, labels: Dict[str, int] = None) -> Instance:  # type: ignore\n",
    "        # pylint: disable=arguments-differ\n",
    "        sentences = [i.string.strip() for i in self._sentence_splitter(abstract_text).sents]\n",
    "        tokenized_abstract_text = [self._tokenizer.tokenize(i) for i in sentences]\n",
    "        abstract_text_field = ListField([TextField(i, self._token_indexers) for i in tokenized_abstract_text])\n",
    "        fields = {'abstract_text': abstract_text_field}\n",
    "        \n",
    "        if labels is not None:\n",
    "            fields['label'] = MultiLabelField([label for label,value in labels.items() if value == 1])\n",
    "                \n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtdHAN(Model):\n",
    "    \"\"\"\n",
    "    vocab : ``Vocabulary``, required\n",
    "        A Vocabulary, required in order to compute sizes for input/output projections.\n",
    "    text_field_embedder : ``TextFieldEmbedder``, required\n",
    "        Used to embed the ``tokens`` ``TextField`` we get as input to the model.\n",
    "    abstract_encoder : ``Seq2VecEncoder``\n",
    "        The encoder that we will use to convert the abstract to a vector.\n",
    "    classifier_feedforward : ``FeedForward``\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 word_encoder: Seq2SeqEncoder,\n",
    "                 sentence_encoder: Seq2SeqEncoder,\n",
    "                 classifier_feedforward: Union[FeedForward, Maxout],\n",
    "                 attended_text_dropout: float  = 0.0,\n",
    "                 bce_pos_weight: int = 10,\n",
    "                 use_positional_encoding: bool = False,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(EtdHAN, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.word_encoder = word_encoder\n",
    "        self.word_level_attention = FeedForward(word_encoder.get_output_dim(), \n",
    "                                                2, \n",
    "                                                [word_encoder.get_output_dim(), 1],\n",
    "                                                [Activation.by_name(\"tanh\")(), Activation.by_name(\"linear\")()],\n",
    "                                                [True, False])\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.sentence_level_attention = FeedForward(sentence_encoder.get_output_dim(), \n",
    "                                                    2, \n",
    "                                                    [sentence_encoder.get_output_dim(), 1],\n",
    "                                                    [Activation.by_name(\"tanh\")(), Activation.by_name(\"linear\")()],\n",
    "                                                    [True, False])\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "\n",
    "        self._dropout = torch.nn.Dropout(attended_text_dropout)\n",
    "        \n",
    "        if text_field_embedder.get_output_dim() != word_encoder.get_input_dim():\n",
    "            raise ConfigurationError(\"The output dimension of the text_field_embedder must match the \"\n",
    "                                     \"input dimension of the word_encoder. Found {} and {}, \"\n",
    "                                     \"respectively.\".format(text_field_embedder.get_output_dim(),\n",
    "                                                            word_encoder.get_input_dim()))\n",
    "\n",
    "        self.metrics = {\n",
    "#             \"roc_auc_score\": RocAucScore()            \n",
    "            \"hit_5\": HitAtK(5),\n",
    "            \"hit_10\": HitAtK(10),\n",
    "            \"precision_5\": PrecisionAtK(5),\n",
    "            \"precision_10\": PrecisionAtK(10)\n",
    "#             \"hit_100\": HitAtK(100),\n",
    "#             \"macro_measure\": MacroF1Measure(top_k=5,num_label=self.num_classes)\n",
    "        }\n",
    "        \n",
    "        self.loss = torch.nn.BCEWithLogitsLoss(pos_weight = torch.ones(self.num_classes)*bce_pos_weight)\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                abstract_text: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        embedded_abstract_text = self.text_field_embedder(abstract_text, num_wrapping_dims=1)\n",
    "        abstract_text_mask = util.get_text_field_mask(abstract_text, num_wrapping_dims=1)\n",
    "        \n",
    "#         print(embedded_abstract_text.size())\n",
    "        num_instance, num_sentence, num_word, _ = embedded_abstract_text.size()\n",
    "        embedded_abstract_text = embedded_abstract_text.view(num_instance*num_sentence, num_word, -1)\n",
    "        abstract_text_mask = abstract_text_mask.view(num_instance*num_sentence, -1)\n",
    "\n",
    "        if self.use_positional_encoding:\n",
    "            embedded_abstract_text = util.add_positional_features(embedded_abstract_text)\n",
    "        encoded_abstract_text = self.word_encoder(embedded_abstract_text, abstract_text_mask)\n",
    "        \n",
    "        word_attentive_weights = self.word_level_attention(encoded_abstract_text)\n",
    "        word_attentive_weights = util.masked_softmax(word_attentive_weights.transpose(1,2), \n",
    "                                                     abstract_text_mask, dim=2).transpose(1,2)\n",
    "        attended_sentences = util.weighted_sum(encoded_abstract_text,word_attentive_weights.squeeze())\n",
    "        \n",
    "        attended_sentences = attended_sentences.view(num_instance, num_sentence, -1)\n",
    "        abstract_text_mask = abstract_text_mask.view(num_instance, num_sentence, -1).sum(2).ge(1).long()\n",
    "        \n",
    "        if self.use_positional_encoding:\n",
    "            attended_sentences = util.add_positional_features(attended_sentences)\n",
    "        attended_sentences = self.sentence_encoder(attended_sentences, abstract_text_mask)\n",
    "        \n",
    "        sentence_attentive_weights = self.sentence_level_attention(attended_sentences)\n",
    "        sentence_attentive_weights = util.masked_softmax(sentence_attentive_weights.transpose(1,2), \n",
    "                                                         abstract_text_mask, dim=2).transpose(1,2)\n",
    "        attended_abstract_text = util.weighted_sum(attended_sentences,sentence_attentive_weights.squeeze())\n",
    "        attended_abstract_text = self._dropout(attended_abstract_text)\n",
    "        \n",
    "        outputs = self.classifier_feedforward(attended_abstract_text)\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        logits = logits.unsqueeze(0) if len(logits.size()) < 2 else logits\n",
    "        output_dict = {'logits': logits}\n",
    "\n",
    "        if label is not None:\n",
    "            outputs = outputs.unsqueeze(0) if len(outputs.size()) < 2 else outputs\n",
    "            loss = self.loss(outputs, label.squeeze(-1))\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label.squeeze(-1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "            \n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Does a simple argmax over the class probabilities, converts indices to string labels, and\n",
    "        adds a ``\"label\"`` key to the dictionary with the result.\n",
    "        \"\"\"\n",
    "        # class_probabilities = output_dict['logits']\n",
    "        # output_dict['class_probabilities'] = class_probabilities\n",
    "\n",
    "        # predictions = class_probabilities.cpu().data.numpy()\n",
    "        # labels = [list(self.vocab.get_index_to_token_vocabulary(namespace=\"labels\").values()) for i in range(len(output_dict['logits']))]\n",
    "        # output_dict['label'] = labels\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "#         metric_dict = {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n",
    "        metric_dict = {}\n",
    "        metric_dict['hit_5'] = self.metrics['hit_5'].get_metric(reset)\n",
    "        metric_dict['hit_10'] = self.metrics['hit_10'].get_metric(reset)\n",
    "#         metric_dict['hit_100'] = self.metrics['hit_100'].get_metric(reset)\n",
    "        metric_dict['precision_5'] = self.metrics['precision_5'].get_metric(reset)\n",
    "        metric_dict['precision_10'] = self.metrics['precision_10'].get_metric(reset)\n",
    "#         macro_measure = self.metrics['macro_measure'].get_metric(reset)\n",
    "#         metric_dict['mac_prec'] = macro_measure[0]\n",
    "#         metric_dict['mac_rec'] = macro_measure[1]\n",
    "#         metric_dict['mac_f1'] = macro_measure[2]\n",
    "        return metric_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'EtdHAN':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab=vocab, params=embedder_params)\n",
    "        word_encoder = Seq2SeqEncoder.from_params(params.pop(\"word_encoder\"))\n",
    "        sentence_encoder = Seq2SeqEncoder.from_params(params.pop(\"sentence_encoder\"))\n",
    "        classifier_feedforward = params.pop(\"classifier_feedforward\")\n",
    "        if classifier_feedforward.pop('type') == 'feedforward':\n",
    "            classifier_feedforward = FeedForward.from_params(classifier_feedforward)\n",
    "        else:\n",
    "            classifier_feedforward = Maxout.from_params(classifier_feedforward)\n",
    "        use_positional_encoding = params.pop(\"use_positional_encoding\", False)\n",
    "        bce_pos_weight = params.pop_int(\"bce_pos_weight\", 10)\n",
    "        attended_text_dropout = params.pop_float(\"attended_text_dropout\", 0.0)\n",
    "\n",
    "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
    "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
    "\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   word_encoder=word_encoder,\n",
    "                   sentence_encoder=sentence_encoder,\n",
    "                   classifier_feedforward=classifier_feedforward,\n",
    "                   attended_text_dropout=attended_text_dropout,\n",
    "                   bce_pos_weight=bce_pos_weight,\n",
    "                   use_positional_encoding=use_positional_encoding,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = EtdHANAbstractReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "4it [00:00, 28.75it/s]\u001b[A\n",
      "10it [00:00, 36.53it/s]\u001b[A\n",
      "16it [00:00, 41.35it/s]\u001b[A\n",
      "21it [00:00, 42.37it/s]\u001b[A\n",
      "26it [00:00, 43.64it/s]\u001b[A\n",
      "31it [00:00, 44.54it/s]\u001b[A\n",
      "37it [00:00, 45.98it/s]\u001b[A\n",
      "42it [00:00, 44.75it/s]\u001b[A\n",
      "48it [00:01, 45.62it/s]\u001b[A\n",
      "53it [00:01, 45.62it/s]\u001b[A\n",
      "58it [00:01, 45.82it/s]\u001b[A\n",
      "63it [00:01, 45.72it/s]\u001b[A\n",
      "69it [00:01, 45.87it/s]\u001b[A\n",
      "75it [00:01, 46.58it/s]\u001b[A\n",
      "81it [00:01, 47.28it/s]\u001b[A\n",
      "87it [00:01, 47.40it/s]\u001b[A\n",
      "93it [00:01, 47.73it/s]\u001b[A\n",
      "99it [00:02, 47.26it/s]\u001b[A\n",
      "105it [00:02, 47.78it/s]\u001b[A\n",
      "111it [00:02, 47.55it/s]\u001b[A\n",
      "117it [00:02, 47.93it/s]\u001b[A\n",
      "123it [00:02, 47.75it/s]\u001b[A\n",
      "130it [00:02, 48.23it/s]\u001b[A\n",
      "136it [00:02, 47.99it/s]\u001b[A\n",
      "141it [00:02, 47.88it/s]\u001b[A\n",
      "146it [00:03, 47.90it/s]\u001b[A\n",
      "152it [00:03, 48.08it/s]\u001b[A\n",
      "159it [00:03, 48.62it/s]\u001b[A\n",
      "165it [00:03, 48.54it/s]\u001b[A\n",
      "171it [00:03, 48.40it/s]\u001b[A\n",
      "177it [00:03, 48.57it/s]\u001b[A\n",
      "183it [00:03, 48.58it/s]\u001b[A\n",
      "189it [00:03, 48.19it/s]\u001b[A\n",
      "194it [00:04, 48.09it/s]\u001b[A\n",
      "200it [00:04, 48.33it/s]\u001b[A\n",
      "205it [00:04, 48.24it/s]\u001b[A\n",
      "210it [00:04, 48.19it/s]\u001b[A\n",
      "215it [00:04, 48.23it/s]\u001b[A\n",
      "220it [00:04, 47.94it/s]\u001b[A\n",
      "225it [00:04, 47.83it/s]\u001b[A\n",
      "231it [00:04, 48.06it/s]\u001b[A\n",
      "237it [00:04, 48.10it/s]\u001b[A\n",
      "242it [00:05, 48.11it/s]\u001b[A\n",
      "249it [00:05, 48.50it/s]\u001b[A\n",
      "255it [00:05, 48.47it/s]\u001b[A\n",
      "262it [00:05, 48.73it/s]\u001b[A\n",
      "268it [00:05, 48.71it/s]\u001b[A\n",
      "274it [00:05, 48.89it/s]\u001b[A\n",
      "280it [00:05, 48.82it/s]\u001b[A\n",
      "286it [00:05, 48.65it/s]\u001b[A\n",
      "292it [00:05, 48.67it/s]\u001b[A\n",
      "297it [00:06, 48.68it/s]\u001b[A\n",
      "303it [00:06, 48.75it/s]\u001b[A\n",
      "309it [00:06, 48.76it/s]\u001b[A\n",
      "315it [00:06, 48.90it/s]\u001b[A\n",
      "321it [00:06, 48.98it/s]\u001b[A\n",
      "327it [00:06, 48.97it/s]\u001b[A\n",
      "333it [00:06, 48.53it/s]\u001b[A\n",
      "339it [00:06, 48.62it/s]\u001b[A\n",
      "344it [00:07, 48.53it/s]\u001b[A\n",
      "349it [00:07, 48.37it/s]\u001b[A\n",
      "355it [00:07, 48.45it/s]\u001b[A\n",
      "361it [00:07, 48.53it/s]\u001b[A\n",
      "366it [00:07, 48.32it/s]\u001b[A\n",
      "372it [00:07, 48.46it/s]\u001b[A\n",
      "377it [00:07, 48.43it/s]\u001b[A\n",
      "382it [00:07, 48.45it/s]\u001b[A\n",
      "387it [00:07, 48.45it/s]\u001b[A\n",
      "392it [00:08, 48.35it/s]\u001b[A\n",
      "398it [00:08, 48.43it/s]\u001b[A\n",
      "405it [00:08, 48.65it/s]\u001b[A\n",
      "411it [00:08, 48.68it/s]\u001b[A\n",
      "417it [00:08, 48.59it/s]\u001b[A\n",
      "423it [00:08, 48.66it/s]\u001b[A\n",
      "429it [00:08, 48.65it/s]\u001b[A\n",
      "435it [00:08, 48.62it/s]\u001b[A\n",
      "440it [00:09, 48.60it/s]\u001b[A\n",
      "446it [00:09, 48.69it/s]\u001b[A\n",
      "453it [00:09, 48.92it/s]\u001b[A\n",
      "459it [00:09, 48.86it/s]\u001b[A\n",
      "465it [00:09, 48.90it/s]\u001b[A\n",
      "472it [00:09, 49.11it/s]\u001b[A\n",
      "478it [00:09, 49.18it/s]\u001b[A\n",
      "485it [00:09, 49.26it/s]\u001b[A\n",
      "491it [00:09, 49.30it/s]\u001b[A\n",
      "497it [00:10, 49.22it/s]\u001b[A\n",
      "503it [00:10, 49.22it/s]\u001b[A\n",
      "511it [00:10, 49.48it/s]\u001b[A\n",
      "518it [00:10, 49.66it/s]\u001b[A\n",
      "525it [00:10, 49.69it/s]\u001b[A\n",
      "531it [00:10, 49.70it/s]\u001b[A\n",
      "537it [00:10, 49.75it/s]\u001b[A\n",
      "543it [00:10, 49.75it/s]\u001b[A\n",
      "549it [00:11, 49.66it/s]\u001b[A\n",
      "555it [00:11, 49.73it/s]\u001b[A\n",
      "561it [00:11, 49.72it/s]\u001b[A\n",
      "567it [00:11, 49.74it/s]\u001b[A\n",
      "573it [00:11, 49.77it/s]\u001b[A\n",
      "579it [00:11, 49.84it/s]\u001b[A\n",
      "585it [00:11, 49.85it/s]\u001b[A\n",
      "592it [00:11, 49.96it/s]\u001b[A\n",
      "598it [00:11, 49.94it/s]\u001b[A\n",
      "604it [00:12, 49.83it/s]\u001b[A\n",
      "609it [00:12, 49.80it/s]\u001b[A\n",
      "616it [00:12, 49.87it/s]\u001b[A\n",
      "622it [00:12, 49.84it/s]\u001b[A\n",
      "627it [00:12, 49.73it/s]\u001b[A\n",
      "633it [00:12, 49.79it/s]\u001b[A\n",
      "638it [00:12, 49.67it/s]\u001b[A\n",
      "643it [00:12, 49.65it/s]\u001b[A\n",
      "648it [00:13, 49.46it/s]\u001b[A\n",
      "653it [00:13, 49.45it/s]\u001b[A\n",
      "659it [00:13, 49.47it/s]\u001b[A\n",
      "664it [00:13, 49.47it/s]\u001b[A\n",
      "670it [00:13, 49.47it/s]\u001b[A\n",
      "676it [00:13, 49.51it/s]\u001b[A\n",
      "681it [00:13, 49.51it/s]\u001b[A\n",
      "686it [00:13, 49.47it/s]\u001b[A\n",
      "692it [00:13, 49.52it/s]\u001b[A\n",
      "698it [00:14, 49.39it/s]\u001b[A\n",
      "703it [00:14, 49.38it/s]\u001b[A\n",
      "709it [00:14, 49.38it/s]\u001b[A\n",
      "715it [00:14, 49.39it/s]\u001b[A\n",
      "720it [00:14, 49.36it/s]\u001b[A\n",
      "726it [00:14, 49.43it/s]\u001b[A\n",
      "732it [00:14, 49.36it/s]\u001b[A\n",
      "737it [00:14, 49.34it/s]\u001b[A\n",
      "743it [00:15, 49.36it/s]\u001b[A\n",
      "748it [00:15, 49.36it/s]\u001b[A\n",
      "754it [00:15, 49.38it/s]\u001b[A\n",
      "760it [00:15, 49.41it/s]\u001b[A\n",
      "766it [00:15, 49.43it/s]\u001b[A\n",
      "772it [00:15, 49.40it/s]\u001b[A\n",
      "779it [00:15, 49.49it/s]\u001b[A\n",
      "785it [00:15, 49.40it/s]\u001b[A\n",
      "793it [00:15, 49.56it/s]\u001b[A\n",
      "799it [00:16, 49.55it/s]\u001b[A\n",
      "805it [00:16, 49.51it/s]\u001b[A\n",
      "811it [00:16, 49.47it/s]\u001b[A\n",
      "816it [00:16, 49.47it/s]\u001b[A\n",
      "823it [00:16, 49.53it/s]\u001b[A\n",
      "829it [00:16, 49.55it/s]\u001b[A\n",
      "835it [00:16, 49.47it/s]\u001b[A\n",
      "841it [00:16, 49.49it/s]\u001b[A\n",
      "847it [00:17, 49.51it/s]\u001b[A\n",
      "853it [00:17, 49.51it/s]\u001b[A\n",
      "859it [00:17, 49.51it/s]\u001b[A\n",
      "865it [00:17, 49.51it/s]\u001b[A\n",
      "870it [00:17, 49.47it/s]\u001b[A\n",
      "875it [00:17, 49.44it/s]\u001b[A\n",
      "881it [00:17, 49.42it/s]\u001b[A\n",
      "887it [00:17, 49.44it/s]\u001b[A\n",
      "893it [00:18, 49.48it/s]\u001b[A\n",
      "899it [00:18, 49.44it/s]\u001b[A\n",
      "904it [00:18, 49.42it/s]\u001b[A\n",
      "909it [00:18, 49.40it/s]\u001b[A\n",
      "915it [00:18, 49.43it/s]\u001b[A\n",
      "922it [00:18, 49.49it/s]\u001b[A\n",
      "928it [00:18, 49.46it/s]\u001b[A\n",
      "934it [00:18, 49.50it/s]\u001b[A\n",
      "940it [00:18, 49.54it/s]\u001b[A\n",
      "946it [00:19, 49.57it/s]\u001b[A\n",
      "952it [00:19, 49.60it/s]\u001b[A\n",
      "955it [00:19, 49.61it/s]\u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "6it [00:00, 56.90it/s]\u001b[A\n",
      "11it [00:00, 52.91it/s]\u001b[A\n",
      "17it [00:00, 51.33it/s]\u001b[A\n",
      "23it [00:00, 51.82it/s]\u001b[A\n",
      "29it [00:00, 52.49it/s]\u001b[A\n",
      "36it [00:00, 54.34it/s]\u001b[A\n",
      "41it [00:00, 51.98it/s]\u001b[A\n",
      "46it [00:00, 51.62it/s]\u001b[A\n",
      "52it [00:00, 52.10it/s]\u001b[A\n",
      "58it [00:01, 52.07it/s]\u001b[A\n",
      "64it [00:01, 51.39it/s]\u001b[A\n",
      "69it [00:01, 45.84it/s]\u001b[A\n",
      "76it [00:01, 46.90it/s]\u001b[A\n",
      "83it [00:01, 47.88it/s]\u001b[A\n",
      "89it [00:01, 48.44it/s]\u001b[A\n",
      "95it [00:01, 48.51it/s]\u001b[A\n",
      "101it [00:02, 48.35it/s]\u001b[A\n",
      "107it [00:02, 48.56it/s]\u001b[A\n",
      "113it [00:02, 48.69it/s]\u001b[A\n",
      "119it [00:02, 49.11it/s]\u001b[A\n",
      "125it [00:02, 49.24it/s]\u001b[A\n",
      "131it [00:02, 49.46it/s]\u001b[A\n",
      "137it [00:02, 49.31it/s]\u001b[A\n",
      "143it [00:02, 49.46it/s]\u001b[A\n",
      "150it [00:03, 49.94it/s]\u001b[A\n",
      "156it [00:03, 50.05it/s]\u001b[A\n",
      "162it [00:03, 50.02it/s]\u001b[A\n",
      "169it [00:03, 50.23it/s]\u001b[A\n",
      "175it [00:03, 50.19it/s]\u001b[A\n",
      "181it [00:03, 50.27it/s]\u001b[A\n",
      "187it [00:03, 49.87it/s]\u001b[A\n",
      "192it [00:03, 49.76it/s]\u001b[A\n",
      "198it [00:03, 50.00it/s]\u001b[A\n",
      "204it [00:04, 50.09it/s]\u001b[A\n",
      "210it [00:04, 50.00it/s]\u001b[A\n",
      "216it [00:04, 50.15it/s]\u001b[A\n",
      "222it [00:04, 49.60it/s]\u001b[A\n",
      "227it [00:04, 49.60it/s]\u001b[A\n",
      "234it [00:04, 49.80it/s]\u001b[A\n",
      "241it [00:04, 50.00it/s]\u001b[A\n",
      "247it [00:04, 50.12it/s]\u001b[A\n",
      "254it [00:05, 50.40it/s]\u001b[A\n",
      "260it [00:05, 50.56it/s]\u001b[A\n",
      "266it [00:05, 50.48it/s]\u001b[A\n",
      "272it [00:05, 50.59it/s]\u001b[A\n",
      "278it [00:05, 50.59it/s]\u001b[A\n",
      "284it [00:05, 50.72it/s]\u001b[A\n",
      "290it [00:05, 50.51it/s]\u001b[A\n",
      "296it [00:05, 50.58it/s]\u001b[A\n",
      "302it [00:05, 50.60it/s]\u001b[A\n",
      "308it [00:06, 50.61it/s]\u001b[A\n",
      "314it [00:06, 50.71it/s]\u001b[A\n",
      "321it [00:06, 50.88it/s]\u001b[A\n",
      "327it [00:06, 50.87it/s]\u001b[A\n",
      "333it [00:06, 50.42it/s]\u001b[A\n",
      "339it [00:06, 50.52it/s]\u001b[A\n",
      "345it [00:06, 50.41it/s]\u001b[A\n",
      "350it [00:06, 50.34it/s]\u001b[A\n",
      "356it [00:07, 50.39it/s]\u001b[A\n",
      "362it [00:07, 50.32it/s]\u001b[A\n",
      "367it [00:07, 50.23it/s]\u001b[A\n",
      "374it [00:07, 50.33it/s]\u001b[A\n",
      "380it [00:07, 50.44it/s]\u001b[A\n",
      "386it [00:07, 50.43it/s]\u001b[A\n",
      "392it [00:07, 50.34it/s]\u001b[A\n",
      "398it [00:07, 50.43it/s]\u001b[A\n",
      "405it [00:07, 50.66it/s]\u001b[A\n",
      "411it [00:08, 50.68it/s]\u001b[A\n",
      "417it [00:08, 50.59it/s]\u001b[A\n",
      "423it [00:08, 50.67it/s]\u001b[A\n",
      "429it [00:08, 50.65it/s]\u001b[A\n",
      "435it [00:08, 50.62it/s]\u001b[A\n",
      "441it [00:08, 50.64it/s]\u001b[A\n",
      "448it [00:08, 50.82it/s]\u001b[A\n",
      "455it [00:08, 50.95it/s]\u001b[A\n",
      "461it [00:09, 50.83it/s]\u001b[A\n",
      "468it [00:09, 51.02it/s]\u001b[A\n",
      "475it [00:09, 51.16it/s]\u001b[A\n",
      "482it [00:09, 51.30it/s]\u001b[A\n",
      "489it [00:09, 51.38it/s]\u001b[A\n",
      "495it [00:09, 51.30it/s]\u001b[A\n",
      "501it [00:09, 51.20it/s]\u001b[A\n",
      "510it [00:09, 51.51it/s]\u001b[A\n",
      "517it [00:10, 51.67it/s]\u001b[A\n",
      "524it [00:10, 51.73it/s]\u001b[A\n",
      "531it [00:10, 51.78it/s]\u001b[A\n",
      "537it [00:10, 51.83it/s]\u001b[A\n",
      "543it [00:10, 51.86it/s]\u001b[A\n",
      "549it [00:10, 51.75it/s]\u001b[A\n",
      "555it [00:10, 51.82it/s]\u001b[A\n",
      "561it [00:10, 51.81it/s]\u001b[A\n",
      "567it [00:10, 51.82it/s]\u001b[A\n",
      "573it [00:11, 51.85it/s]\u001b[A\n",
      "579it [00:11, 51.92it/s]\u001b[A\n",
      "585it [00:11, 51.93it/s]\u001b[A\n",
      "591it [00:11, 52.00it/s]\u001b[A\n",
      "597it [00:11, 51.97it/s]\u001b[A\n",
      "603it [00:11, 51.86it/s]\u001b[A\n",
      "609it [00:11, 51.74it/s]\u001b[A\n",
      "616it [00:11, 51.78it/s]\u001b[A\n",
      "622it [00:12, 51.71it/s]\u001b[A\n",
      "627it [00:12, 51.57it/s]\u001b[A\n",
      "634it [00:12, 51.63it/s]\u001b[A\n",
      "639it [00:12, 51.53it/s]\u001b[A\n",
      "644it [00:12, 51.50it/s]\u001b[A\n",
      "649it [00:12, 51.29it/s]\u001b[A\n",
      "654it [00:12, 51.21it/s]\u001b[A\n",
      "660it [00:12, 51.17it/s]\u001b[A\n",
      "667it [00:13, 51.20it/s]\u001b[A\n",
      "672it [00:13, 51.14it/s]\u001b[A\n",
      "678it [00:13, 51.19it/s]\u001b[A\n",
      "683it [00:13, 51.09it/s]\u001b[A\n",
      "689it [00:13, 51.14it/s]\u001b[A\n",
      "695it [00:13, 51.01it/s]\u001b[A\n",
      "700it [00:13, 50.98it/s]\u001b[A\n",
      "706it [00:13, 51.01it/s]\u001b[A\n",
      "711it [00:13, 50.91it/s]\u001b[A\n",
      "717it [00:14, 50.93it/s]\u001b[A\n",
      "723it [00:14, 50.96it/s]\u001b[A\n",
      "729it [00:14, 50.96it/s]\u001b[A\n",
      "735it [00:14, 50.91it/s]\u001b[A\n",
      "740it [00:14, 50.88it/s]\u001b[A\n",
      "746it [00:14, 50.87it/s]\u001b[A\n",
      "752it [00:14, 50.87it/s]\u001b[A\n",
      "758it [00:14, 50.93it/s]\u001b[A\n",
      "764it [00:15, 50.92it/s]\u001b[A\n",
      "770it [00:15, 50.97it/s]\u001b[A\n",
      "776it [00:15, 50.95it/s]\u001b[A\n",
      "783it [00:15, 51.00it/s]\u001b[A\n",
      "789it [00:15, 51.04it/s]\u001b[A\n",
      "795it [00:15, 51.04it/s]\u001b[A\n",
      "801it [00:15, 51.01it/s]\u001b[A\n",
      "807it [00:15, 50.98it/s]\u001b[A\n",
      "813it [00:15, 50.96it/s]\u001b[A\n",
      "820it [00:16, 51.00it/s]\u001b[A\n",
      "827it [00:16, 51.03it/s]\u001b[A\n",
      "833it [00:16, 50.98it/s]\u001b[A\n",
      "839it [00:16, 50.86it/s]\u001b[A\n",
      "845it [00:16, 50.88it/s]\u001b[A\n",
      "851it [00:16, 50.84it/s]\u001b[A\n",
      "856it [00:16, 50.70it/s]\u001b[A\n",
      "861it [00:16, 50.67it/s]\u001b[A\n",
      "866it [00:17, 50.60it/s]\u001b[A\n",
      "872it [00:17, 50.58it/s]\u001b[A\n",
      "877it [00:17, 50.55it/s]\u001b[A\n",
      "882it [00:17, 50.47it/s]\u001b[A\n",
      "887it [00:17, 50.42it/s]\u001b[A\n",
      "892it [00:17, 50.37it/s]\u001b[A\n",
      "897it [00:17, 50.28it/s]\u001b[A\n",
      "902it [00:17, 50.12it/s]\u001b[A\n",
      "906it [00:18, 50.04it/s]\u001b[A\n",
      "913it [00:18, 50.15it/s]\u001b[A\n",
      "920it [00:18, 50.22it/s]\u001b[A\n",
      "926it [00:18, 50.23it/s]\u001b[A\n",
      "932it [00:18, 50.26it/s]\u001b[A\n",
      "938it [00:18, 50.26it/s]\u001b[A\n",
      "945it [00:18, 50.34it/s]\u001b[A\n",
      "951it [00:18, 50.38it/s]\u001b[A\n",
      "955it [00:18, 50.41it/s]\u001b[A12/31/2018 17:10:37 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "\n",
      "  0%|          | 0/1910 [00:00<?, ?it/s]\u001b[A\n",
      " 26%|██▌       | 492/1910 [00:00<00:00, 4884.91it/s]\u001b[A\n",
      " 52%|█████▏    | 996/1910 [00:00<00:00, 4959.25it/s]\u001b[A\n",
      " 82%|████████▏ | 1557/1910 [00:00<00:00, 5173.75it/s]\u001b[A\n",
      "100%|██████████| 1910/1910 [00:00<00:00, 5165.69it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "train_dataset = reader.read(cached_path(\n",
    "    \"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\"))\n",
    "validation_dataset = reader.read(cached_path(\n",
    "    \"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\"))\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.0.bias\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      classifier_feedforward._linear_layers.0.weight\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.bias_hh_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.bias_hh_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.bias_ih_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.bias_ih_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.weight_hh_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.weight_hh_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.weight_ih_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_encoder._module.weight_ih_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_level_attention._linear_layers.0.bias\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_level_attention._linear_layers.0.weight\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_level_attention._linear_layers.1.bias\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      sentence_level_attention._linear_layers.1.weight\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.bias_hh_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.bias_hh_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.bias_ih_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.bias_ih_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.weight_hh_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.weight_hh_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.weight_ih_l0\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_encoder._module.weight_ih_l0_reverse\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_level_attention._linear_layers.0.bias\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_level_attention._linear_layers.0.weight\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_level_attention._linear_layers.1.bias\n",
      "12/31/2018 17:10:38 - INFO - allennlp.nn.initializers -      word_level_attention._linear_layers.1.weight\n"
     ]
    }
   ],
   "source": [
    "word_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=True, batch_first=True))\n",
    "sentence_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(12, HIDDEN_DIM, bidirectional=True, batch_first=True))\n",
    "ff = FeedForward(12,1,1714,Activation.by_name(\"linear\")())\n",
    "model = EtdHAN(vocab,\n",
    "               text_field_embedder=word_embeddings, \n",
    "               word_encoder=word_lstm,\n",
    "               sentence_encoder=sentence_lstm,\n",
    "#                attention_encoder=attention_encoder,\n",
    "               classifier_feedforward=ff)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# iterator = BucketIterator(batch_size=16, sorting_keys=[(\"abstract_text\", \"list_num_tokens\"),(\"abstract_text\", \"num_fields\")])\n",
    "# iterator = BucketIterator(batch_size=16, sorting_keys=[(\"abstract_text\", \"num_fields\")])\n",
    "iterator = BucketIterator(batch_size=1, sorting_keys=[(\"abstract_text\", \"list_num_tokens\")])\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_text = {\"tokens\": torch.tensor([[[1,2,3,4,0,0],[1,2,0,0,0,0]],[[0,0,0,0,0,0],[1,2,1,6,0,0]]]).long()}\n",
    "x,mask = model.text_field_embedder(abstract_text), util.get_text_field_mask(abstract_text, num_wrapping_dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9800e-03,  9.8423e-03,  2.6894e-03, -2.3688e-03,  5.3659e-03,\n",
       "          9.5438e-04],\n",
       "        [ 7.0658e-03, -7.5392e-03, -1.4105e-03, -1.7417e-03,  2.8041e-03,\n",
       "         -7.7285e-05],\n",
       "        [ 1.6912e-02,  6.0203e-03, -7.2046e-03, -1.1684e-02, -3.2427e-03,\n",
       "         -8.3448e-03],\n",
       "        [-6.1995e-03,  9.2633e-04,  1.1533e-02,  1.0515e-02, -1.8912e-02,\n",
       "          1.1993e-02],\n",
       "        [ 9.9684e-03, -8.5390e-03, -3.7505e-03,  9.0805e-03, -1.8213e-02,\n",
       "          1.1602e-02],\n",
       "        [ 9.9684e-03, -8.5390e-03, -3.7505e-03,  9.0805e-03, -1.8213e-02,\n",
       "          1.1602e-02]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9800e-03,  9.8423e-03,  2.6894e-03, -2.3688e-03,  5.3659e-03,\n",
       "          9.5438e-04],\n",
       "        [ 7.0658e-03, -7.5392e-03, -1.4105e-03, -1.7417e-03,  2.8041e-03,\n",
       "         -7.7285e-05],\n",
       "        [ 1.6912e-02,  6.0203e-03, -7.2046e-03, -1.1684e-02, -3.2427e-03,\n",
       "         -8.3448e-03],\n",
       "        [-6.1995e-03,  9.2633e-04,  1.1533e-02,  1.0515e-02, -1.8912e-02,\n",
       "          1.1993e-02],\n",
       "        [ 9.9684e-03, -8.5390e-03, -3.7505e-03,  9.0805e-03, -1.8213e-02,\n",
       "          1.1602e-02],\n",
       "        [ 9.9684e-03, -8.5390e-03, -3.7505e-03,  9.0805e-03, -1.8213e-02,\n",
       "          1.1602e-02]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 6])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 19])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(2).ge(1).long().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object EtdAbstractReader._read at 0x7f9370bb8eb8>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader._read(cached_path(\n",
    "    \"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataIterator.__call__ at 0x7f93719f1f10>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator(reader._read(cached_path(\"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am an apple', 'you are a boy']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'@@@sent@@@'.join(['i am an apple','you are a boy']).split(\"@@@sent@@@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,10)\n",
    "mask = torch.randn(3,10).ge(0.5).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 0, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-d6201e6af21c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.LongTensor for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "x*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b= x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "hidden_dim = 5\n",
    "num_hops = 3\n",
    "\n",
    "x = torch.randn(5,6,input_dim)\n",
    "mask = torch.randn(5,6).ge(0.3).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 0, 1],\n",
       "        [1, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim, False), \n",
    "                                     torch.nn.Tanh(),\n",
    "                                     torch.nn.Linear(hidden_dim, num_hops, False)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = self_attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = util.masked_softmax(A.transpose(1,2), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1309,  0.1582, -0.2105,  1.2122,  0.8495,  0.7268,  0.3333,  1.0941,\n",
       "          1.3880, -0.2298],\n",
       "        [-0.0723,  0.1661, -0.3527,  1.2330,  1.0729,  0.7947,  0.3510,  1.0121,\n",
       "          1.5219, -0.1599],\n",
       "        [ 0.0643,  0.1608, -0.2571,  1.2190,  0.9227,  0.7490,  0.3391,  1.0672,\n",
       "          1.4319, -0.2069]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(A,x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1309,  0.1582, -0.2105,  1.2122,  0.8495,  0.7268,  0.3333,  1.0941,\n",
       "         1.3880, -0.2298, -0.0723,  0.1661, -0.3527,  1.2330,  1.0729,  0.7947,\n",
       "         0.3510,  1.0121,  1.5219, -0.1599,  0.0643,  0.1608, -0.2571,  1.2190,\n",
       "         0.9227,  0.7490,  0.3391,  1.0672,  1.4319, -0.2069],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(A,x).view(-1, num_hops * input_dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3969, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.bmm(A, A.transpose(1,2).contiguous()) - torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4291, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(A, A.transpose(1,2).contiguous())[0] - torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5388, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(torch.norm(i).pow(2) for i in torch.bmm(A, A.transpose(1,2).contiguous()) - torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5388, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = A.size(1)\n",
    "sum(torch.norm(i).pow(2) for i in torch.bmm(A, A.transpose(1,2).contiguous()) - torch.eye(dim, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,10, (5,10)).long()\n",
    "[batch_size, sent_len] = x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.nn.ModuleList(\n",
    "            [torch.nn.Embedding(10, 5, padding_idx=0) for _ in range(2 * 1 + 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding_ = torch.LongTensor([0] * 1).expand(5,1)\n",
    "# x_pad = torch.cat((padding_, x, padding_), dim=1)\n",
    "x_pad = torch.nn.functional.pad(x, pad=(x.size(0),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 9, 4, 0, 2, 1, 3, 8, 2, 8, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 6, 5, 8, 5, 4, 0, 1, 2, 6, 9, 0],\n",
       "        [0, 0, 0, 0, 0, 5, 7, 2, 6, 3, 4, 5, 7, 2, 7, 0],\n",
       "        [0, 0, 0, 0, 0, 4, 8, 9, 8, 6, 8, 8, 4, 3, 8, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 8, 7, 2, 7, 0, 4, 5, 7, 2, 0]])"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [embed(x_pad[:, i:(i + sent_len)]) for i, embed in enumerate(embedding)]\n",
    "x_embed = torch.stack(e, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple = torch.stack([x_embed[:, :, :, 1] * x_embed[:, :, :, i] for i in range(2 * 1 + 1)], dim=3)\n",
    "\n",
    "local_context = torch.mean(multiple, dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 5])"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_context.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 2, 2, 8, 9, 5, 5, 9, 3, 1],\n",
       "        [7, 5, 1, 1, 1, 1, 6, 8, 1, 3],\n",
       "        [8, 4, 8, 5, 1, 5, 8, 9, 5, 3],\n",
       "        [7, 6, 2, 6, 9, 5, 7, 3, 5, 1],\n",
       "        [8, 3, 9, 1, 4, 6, 9, 8, 9, 4]])"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_library.models.etd_attention import AttentionEncoder\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = AttentionEncoder(10)\n",
    "x = torch.randn(2,5,10)\n",
    "mask = torch.randn(2,5).ge(0.5).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "att1 = deepcopy(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0956,  2.3094, -1.5023,  0.5245,  0.7895,  0.6708, -1.0538,\n",
       "           -0.6314, -0.2453, -1.1940],\n",
       "          [-1.7808,  0.4846, -0.9707, -0.4874,  1.9167,  0.4876, -0.9632,\n",
       "           -0.5616,  3.5243,  1.5542],\n",
       "          [ 1.2836,  0.1189,  0.6261, -0.2110, -1.4688,  1.2819,  0.5364,\n",
       "            1.4083, -0.4425, -1.1375],\n",
       "          [-1.1036, -0.4148, -0.1739,  0.0440, -2.1226, -1.9324,  0.9746,\n",
       "           -1.8671, -0.0771, -0.9847],\n",
       "          [-1.3612,  1.1472, -0.1760,  0.3716,  0.8115, -0.0025, -1.3948,\n",
       "           -0.1042, -1.1905,  1.7788]],\n",
       " \n",
       "         [[ 1.4873,  0.5854, -1.1582,  0.0561, -0.9787,  0.2971, -0.0266,\n",
       "           -1.2644, -1.4301, -1.0191],\n",
       "          [-0.9062, -1.6221,  0.4692, -1.4506,  0.3493,  0.1394, -0.6573,\n",
       "            0.8878, -1.4329,  0.0296],\n",
       "          [ 0.7715, -0.3434, -1.8192,  0.8519,  1.3127,  0.6217, -1.0981,\n",
       "            0.4635, -1.2794, -1.5978],\n",
       "          [-0.5934,  0.3613,  0.2303, -0.8669,  1.2360, -0.2357, -0.1621,\n",
       "            0.6322,  0.0945, -0.6047],\n",
       "          [-1.9441,  0.8252,  1.0181,  0.7968,  0.3399,  0.9106,  1.5720,\n",
       "           -0.5048,  0.0890, -0.9511]]]), tensor([[1, 1, 0, 1, 0],\n",
       "         [1, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0969,  1.3949, -0.8407,  0.0925, -0.0282,  0.1934, -0.7274, -0.6720,\n",
       "           0.6004,  0.0064],\n",
       "         [-1.1849, -0.1936, -1.2598, -0.6128,  2.2592,  1.7331, -0.3720,  0.2144,\n",
       "          -3.9588, -4.1431]], grad_fn=<SqueezeBackward0>),\n",
       " tensor([[-1.0969,  1.3949, -0.8407,  0.0925, -0.0282,  0.1934, -0.7274, -0.6720,\n",
       "           0.6004,  0.0064],\n",
       "         [-1.1849, -0.1936, -1.2598, -0.6128,  2.2592,  1.7331, -0.3720,  0.2144,\n",
       "          -3.9588, -4.1431]], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(x, mask),att1(x,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=att1(x,mask).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = [att]\n",
    "for i in range(10-1):\n",
    "    atts.append(deepcopy(att))\n",
    "atts = torch.nn.ModuleList(atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "attended = torch.stack([i(x,mask) for i in atts], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((y,y,y), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attended[:, -1, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 10, 10]))"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), attended.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1495e-01,  3.3855e-01, -1.7647e-01,  3.7805e-02,  4.5963e-01,\n",
      "         3.7307e-01, -2.9234e-01,  1.8206e-01,  9.5223e-02,  5.6805e-02,\n",
      "        -2.1474e-07, -6.8533e-08, -4.1755e-08,  1.2824e-08, -3.1281e-07,\n",
      "        -3.4604e-07,  1.4355e-07, -3.3732e-07, -6.9607e-09, -1.2825e-07])\n",
      "tensor([1.4435e-07])\n"
     ]
    }
   ],
   "source": [
    "for i in att1._self_attention.parameters():\n",
    "    print(i.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0583, -0.5056,  0.7899, -0.8778,  0.9959],\n",
       "         [ 0.0653,  1.0990,  0.5116,  0.2790, -0.0700],\n",
       "         [-1.1550, -1.6063,  0.7777, -1.5428, -0.8389],\n",
       "         [ 0.1843,  0.1167, -1.8287, -0.0539,  0.8898],\n",
       "         [-0.8531,  1.3917,  2.3115,  0.4720,  0.8648],\n",
       "         [-0.2575,  0.4673,  1.7664, -0.8929,  0.1301],\n",
       "         [-0.2149, -0.8013,  1.4624,  0.8096,  1.1394],\n",
       "         [-0.8729,  0.8131,  1.4391, -1.7514,  1.1175],\n",
       "         [ 1.3564,  0.7563, -0.3517, -0.8089,  2.4565],\n",
       "         [-0.1705,  0.4141, -0.6820,  1.4485, -1.1496]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,10,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rule_predictor import RulePredictor\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095464/1095464 [00:03<00:00, 301003.29it/s]\n"
     ]
    }
   ],
   "source": [
    "rule_predictor = RulePredictor()\n",
    "rule_predictor.load('./data/lcsh/rules/variants_rules.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Knowledge and learning'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_predictor.predict_vote('machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416472"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rule_predictor.predict('machine learning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Employees--Training of': 5,\n",
       " 'Employee selection': 5,\n",
       " 'Business enterprises': 5,\n",
       " 'Employees': 4,\n",
       " 'Personnel management': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_predictor.predict_k(etd_json[0]['etdAbstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3,5,10)\n",
    "c = torch.randn(3,10)\n",
    "mask = torch.ones(3,5)\n",
    "mask[1, 3:] = 0\n",
    "mask[2, 1:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7d9984c6d2af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x*c.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-113b1eee7667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "x.bmm(mask.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0],\n",
      "        [0, 1, 1, 0, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 1]]) \n",
      " tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "g = torch.randint(0,2,(5,5))\n",
    "x = torch.randint(0,2,(3,5))\n",
    "print(g,'\\n',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 2, 2, 3],\n",
       "        [1, 2, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.2000, 0.2000, 0.0000, 0.2000],\n",
       "         [0.2000, 0.0000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.0000, 0.2000, 0.2000]],\n",
       "\n",
       "        [[0.0000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.0000, 0.2000, 0.2000, 0.2000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*(x.unsqueeze(-1).expand(3,5,5)*g).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.1516)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.eye(5,5) - 0.2*(x.unsqueeze(-1).expand(3,5,5)*g).float()).inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = torch.nn.Linear(10,5)\n",
    "opt = torch.optim.Adam(ff.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(690.3333, grad_fn=<SumBackward0>)\n",
      "tensor(687.9799, grad_fn=<SumBackward0>)\n",
      "tensor(685.6893, grad_fn=<SumBackward0>)\n",
      "tensor(683.4610, grad_fn=<SumBackward0>)\n",
      "tensor(681.2941, grad_fn=<SumBackward0>)\n",
      "tensor(679.1877, grad_fn=<SumBackward0>)\n",
      "tensor(677.1398, grad_fn=<SumBackward0>)\n",
      "tensor(675.1490, grad_fn=<SumBackward0>)\n",
      "tensor(673.2129, grad_fn=<SumBackward0>)\n",
      "tensor(671.3298, grad_fn=<SumBackward0>)\n",
      "tensor(669.4982, grad_fn=<SumBackward0>)\n",
      "tensor(667.7164, grad_fn=<SumBackward0>)\n",
      "tensor(665.9832, grad_fn=<SumBackward0>)\n",
      "tensor(664.2968, grad_fn=<SumBackward0>)\n",
      "tensor(662.6559, grad_fn=<SumBackward0>)\n",
      "tensor(661.0593, grad_fn=<SumBackward0>)\n",
      "tensor(659.5052, grad_fn=<SumBackward0>)\n",
      "tensor(657.9922, grad_fn=<SumBackward0>)\n",
      "tensor(656.5187, grad_fn=<SumBackward0>)\n",
      "tensor(655.0835, grad_fn=<SumBackward0>)\n",
      "tensor(653.6851, grad_fn=<SumBackward0>)\n",
      "tensor(652.3223, grad_fn=<SumBackward0>)\n",
      "tensor(650.9937, grad_fn=<SumBackward0>)\n",
      "tensor(649.6979, grad_fn=<SumBackward0>)\n",
      "tensor(648.4340, grad_fn=<SumBackward0>)\n",
      "tensor(647.2007, grad_fn=<SumBackward0>)\n",
      "tensor(645.9968, grad_fn=<SumBackward0>)\n",
      "tensor(644.8210, grad_fn=<SumBackward0>)\n",
      "tensor(643.6721, grad_fn=<SumBackward0>)\n",
      "tensor(642.5488, grad_fn=<SumBackward0>)\n",
      "tensor(641.4500, grad_fn=<SumBackward0>)\n",
      "tensor(640.3743, grad_fn=<SumBackward0>)\n",
      "tensor(639.3202, grad_fn=<SumBackward0>)\n",
      "tensor(638.2867, grad_fn=<SumBackward0>)\n",
      "tensor(637.2723, grad_fn=<SumBackward0>)\n",
      "tensor(636.2757, grad_fn=<SumBackward0>)\n",
      "tensor(635.2957, grad_fn=<SumBackward0>)\n",
      "tensor(634.3313, grad_fn=<SumBackward0>)\n",
      "tensor(633.3813, grad_fn=<SumBackward0>)\n",
      "tensor(632.4449, grad_fn=<SumBackward0>)\n",
      "tensor(631.5211, grad_fn=<SumBackward0>)\n",
      "tensor(630.6093, grad_fn=<SumBackward0>)\n",
      "tensor(629.7086, grad_fn=<SumBackward0>)\n",
      "tensor(628.8184, grad_fn=<SumBackward0>)\n",
      "tensor(627.9381, grad_fn=<SumBackward0>)\n",
      "tensor(627.0672, grad_fn=<SumBackward0>)\n",
      "tensor(626.2053, grad_fn=<SumBackward0>)\n",
      "tensor(625.3517, grad_fn=<SumBackward0>)\n",
      "tensor(624.5063, grad_fn=<SumBackward0>)\n",
      "tensor(623.6686, grad_fn=<SumBackward0>)\n",
      "tensor(622.8381, grad_fn=<SumBackward0>)\n",
      "tensor(622.0148, grad_fn=<SumBackward0>)\n",
      "tensor(621.1984, grad_fn=<SumBackward0>)\n",
      "tensor(620.3886, grad_fn=<SumBackward0>)\n",
      "tensor(619.5854, grad_fn=<SumBackward0>)\n",
      "tensor(618.7883, grad_fn=<SumBackward0>)\n",
      "tensor(617.9976, grad_fn=<SumBackward0>)\n",
      "tensor(617.2129, grad_fn=<SumBackward0>)\n",
      "tensor(616.4341, grad_fn=<SumBackward0>)\n",
      "tensor(615.6613, grad_fn=<SumBackward0>)\n",
      "tensor(614.8940, grad_fn=<SumBackward0>)\n",
      "tensor(614.1326, grad_fn=<SumBackward0>)\n",
      "tensor(613.3767, grad_fn=<SumBackward0>)\n",
      "tensor(612.6267, grad_fn=<SumBackward0>)\n",
      "tensor(611.8821, grad_fn=<SumBackward0>)\n",
      "tensor(611.1433, grad_fn=<SumBackward0>)\n",
      "tensor(610.4102, grad_fn=<SumBackward0>)\n",
      "tensor(609.6827, grad_fn=<SumBackward0>)\n",
      "tensor(608.9610, grad_fn=<SumBackward0>)\n",
      "tensor(608.2451, grad_fn=<SumBackward0>)\n",
      "tensor(607.5347, grad_fn=<SumBackward0>)\n",
      "tensor(606.8304, grad_fn=<SumBackward0>)\n",
      "tensor(606.1318, grad_fn=<SumBackward0>)\n",
      "tensor(605.4390, grad_fn=<SumBackward0>)\n",
      "tensor(604.7518, grad_fn=<SumBackward0>)\n",
      "tensor(604.0706, grad_fn=<SumBackward0>)\n",
      "tensor(603.3950, grad_fn=<SumBackward0>)\n",
      "tensor(602.7253, grad_fn=<SumBackward0>)\n",
      "tensor(602.0611, grad_fn=<SumBackward0>)\n",
      "tensor(601.4026, grad_fn=<SumBackward0>)\n",
      "tensor(600.7496, grad_fn=<SumBackward0>)\n",
      "tensor(600.1023, grad_fn=<SumBackward0>)\n",
      "tensor(599.4602, grad_fn=<SumBackward0>)\n",
      "tensor(598.8235, grad_fn=<SumBackward0>)\n",
      "tensor(598.1921, grad_fn=<SumBackward0>)\n",
      "tensor(597.5658, grad_fn=<SumBackward0>)\n",
      "tensor(596.9446, grad_fn=<SumBackward0>)\n",
      "tensor(596.3286, grad_fn=<SumBackward0>)\n",
      "tensor(595.7173, grad_fn=<SumBackward0>)\n",
      "tensor(595.1111, grad_fn=<SumBackward0>)\n",
      "tensor(594.5096, grad_fn=<SumBackward0>)\n",
      "tensor(593.9127, grad_fn=<SumBackward0>)\n",
      "tensor(593.3204, grad_fn=<SumBackward0>)\n",
      "tensor(592.7328, grad_fn=<SumBackward0>)\n",
      "tensor(592.1497, grad_fn=<SumBackward0>)\n",
      "tensor(591.5710, grad_fn=<SumBackward0>)\n",
      "tensor(590.9965, grad_fn=<SumBackward0>)\n",
      "tensor(590.4262, grad_fn=<SumBackward0>)\n",
      "tensor(589.8604, grad_fn=<SumBackward0>)\n",
      "tensor(589.2983, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    o = torch.sigmoid(ff(inputs))\n",
    "    o_size = o.size()\n",
    "    o_inv = (torch.eye(o_size[-1],o_size[-1]) - 0.2*(o.unsqueeze(-1).expand(o_size[0],o_size[-1],o_size[-1])*g.float())).inverse()\n",
    "    loss = o_inv.sum()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0392, 0.2907, 0.0727, 0.0327, 0.0908],\n",
       "         [0.1962, 1.4535, 0.3634, 0.1635, 0.4542],\n",
       "         [0.4142, 0.2907, 1.3227, 0.3452, 0.4033],\n",
       "         [0.2471, 0.3488, 0.0872, 1.0392, 0.1090],\n",
       "         [0.3706, 0.5233, 0.1308, 0.3089, 1.4135]],\n",
       "\n",
       "        [[1.0000, 0.2000, 0.0000, 0.0000, -0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, -0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000, -0.0000],\n",
       "         [0.2000, 0.2400, 0.0000, 1.0000, -0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, -0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, -0.0000],\n",
       "         [0.2500, 0.0000, 1.2500, 0.2500, 0.2500],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, -0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated cache directory found (/projdata3/info_fil/finance/simon_test/.allennlp/datasets).  Please remove this directory from your system to free up space.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "#from allennlp.data.dataset import Dataset\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, MetadataField, ListField\n",
    "from allennlp.data.fields.multilabel_field import MultiLabelField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "# from multi_toxic_label.fields.multi_label_field import MultiLabelField\n",
    "\n",
    "class EtdAbstractReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a CSV-lines file containing abstract only from ETD records\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 rules_dir: str,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 merge_title_abstract: bool = False,\n",
    "                 lazy: bool = False,\n",
    "                 start_tokens: List[str] = [\"<start>\"], \n",
    "                 end_tokens: List[str] = [\"<end>\"]) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer(start_tokens=start_tokens,end_tokens=end_tokens)\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self._merge_title_abstract = merge_title_abstract\n",
    "        \n",
    "        self._idx_to_rule = {}\n",
    "        with open(rules_dir, 'r')  as f:\n",
    "            for i,r in enumerate(f.readlines()):\n",
    "                w,p = r.split(' => ')\n",
    "                self._idx_to_rule[i] = (w.split(' ^ '), p)\n",
    "        \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "            for line in data_file:\n",
    "                line = line.strip(\"\\n\")\n",
    "                if not line:\n",
    "                    continue\n",
    "                etd_json = json.loads(line)\n",
    "                if self._merge_title_abstract and 'etdTitle' in  etd_json:\n",
    "                    abstract = '%s @@@SEP@@@ %s'%(etd_json['etdTitle'],etd_json['etdAbstract'])\n",
    "                else:\n",
    "                    abstract = etd_json['etdAbstract']\n",
    "                labels = etd_json['lcsh']\n",
    "\n",
    "                yield self.text_to_instance(abstract, labels)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, abstract_text: str, labels: Dict[str, int] = None) -> Instance:  # type: ignore\n",
    "        # pylint: disable=arguments-differ\n",
    "        tokenized_abstract_text = self._tokenizer.tokenize(abstract_text)\n",
    "        abstract_text_field = TextField(tokenized_abstract_text, self._token_indexers)\n",
    "        fields = {'abstract_text': abstract_text_field}\n",
    "        \n",
    "        if labels is not None:\n",
    "            fields['label'] = MultiLabelField([label for label,value in labels.items() if value == 1])\n",
    "                \n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/projdata3/info_fil/finance/simon_test/allennlp_official/allennlp/allennlp/service/predictors/__init__.py:23: FutureWarning: allennlp.service.predictors.* has been depreciated. Please use allennlp.predictors.*\n",
      "  \"Please use allennlp.predictors.*\", FutureWarning)\n",
      "/misc/projdata3/info_fil/finance/simon_test/allennlp_official/allennlp/allennlp/service/predictors/predictor.py:6: FutureWarning: allennlp.service.predictors.* has been deprecated. Please use allennlp.predictors.*\n",
      "  \" Please use allennlp.predictors.*\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "import numpy\n",
    "from overrides import overrides\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder, Seq2SeqEncoder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import BooleanAccuracy\n",
    "\n",
    "from my_library.metrics.roc_auc_score import RocAucScore\n",
    "from my_library.metrics.hit_at_k import *\n",
    "from my_library.metrics.macro_f1 import *\n",
    "from my_library.metrics.precision_at_k import *\n",
    "from my_library.models.etd_attention import AttentionEncoder, MultiHeadAttentionEncoder, SelfAttentionEncoder\n",
    "from my_library.models.etd_pooling import Pooling\n",
    "from my_library.models.customized_allennlp.maxout import Maxout\n",
    "\n",
    "class EtdRNN(Model):\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 abstract_text_encoder: Seq2SeqEncoder,\n",
    "                 attention_encoder: Union[AttentionEncoder, MultiHeadAttentionEncoder, SelfAttentionEncoder, Pooling],\n",
    "                 classifier_feedforward: Union[FeedForward, Maxout],\n",
    "                 bce_pos_weight: int = 10,\n",
    "                 use_positional_encoding: bool = False,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(EtdRNN, self).__init__(vocab, regularizer)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.abstract_text_encoder = abstract_text_encoder\n",
    "        self.attention_encoder = attention_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "\n",
    "        if text_field_embedder.get_output_dim() != abstract_text_encoder.get_input_dim():\n",
    "            raise ConfigurationError(\"The output dimension of the text_field_embedder must match the \"\n",
    "                                     \"input dimension of the abstract_text_encoder. Found {} and {}, \"\n",
    "                                     \"respectively.\".format(text_field_embedder.get_output_dim(),\n",
    "                                                            abstract_text_encoder.get_input_dim()))\n",
    "\n",
    "        self.metrics = {\n",
    "\n",
    "        }\n",
    "        \n",
    "        self.loss = torch.nn.BCEWithLogitsLoss(pos_weight = torch.ones(self.num_classes)*bce_pos_weight)\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,  # type: ignore\n",
    "                abstract_text: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        embedded_abstract_text = self.text_field_embedder(abstract_text)\n",
    "        abstract_text_mask = util.get_text_field_mask(abstract_text)\n",
    "        if self.use_positional_encoding:\n",
    "            embedded_abstract_text = util.add_positional_features(embedded_abstract_text)\n",
    "        encoded_abstract_text = self.abstract_text_encoder(embedded_abstract_text, abstract_text_mask)\n",
    "        \n",
    "        attended_abstract_text = self.attention_encoder(encoded_abstract_text, abstract_text_mask)\n",
    "        outputs = self.classifier_feedforward(attended_abstract_text)\n",
    "        logits = torch.sigmoid(outputs)\n",
    "        logits = logits.unsqueeze(0) if logits.dim() < 2 else logits\n",
    "        output_dict = {'logits': logits}\n",
    "\n",
    "        if label is not None:\n",
    "            outputs = outputs.unsqueeze(0) if outputs.dim() < 2 else outputs\n",
    "            loss = self.loss(outputs, label.squeeze(-1))\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label.squeeze(-1))\n",
    "            output_dict[\"loss\"] = loss\n",
    "            \n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        return output_dict\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metric_dict = {}\n",
    "        return metric_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, vocab: Vocabulary, params: Params) -> 'EtdRNN':\n",
    "        embedder_params = params.pop(\"text_field_embedder\")\n",
    "        text_field_embedder = TextFieldEmbedder.from_params(vocab=vocab, params=embedder_params)\n",
    "        abstract_text_encoder = Seq2SeqEncoder.from_params(params.pop(\"abstract_text_encoder\"))\n",
    "        attention_encoder = params.pop(\"attention_encoder\")\n",
    "        attention_type = attention_encoder.pop('type')\n",
    "        if attention_type == 'linear_attention':\n",
    "            attention_encoder = AttentionEncoder.from_params(attention_encoder)\n",
    "        elif attention_type == 'self_attention':\n",
    "            attention_encoder = SelfAttentionEncoder.from_params(attention_encoder)\n",
    "        elif attention_type == 'multi_head':\n",
    "            attention_encoder = MultiHeadAttentionEncoder.from_params(attention_encoder)\n",
    "        else:\n",
    "            attention_encoder = Pooling.from_params(attention_encoder)\n",
    "        classifier_feedforward = params.pop(\"classifier_feedforward\")\n",
    "        if classifier_feedforward.pop('type') == 'feedforward':\n",
    "            classifier_feedforward = FeedForward.from_params(classifier_feedforward)\n",
    "        else:\n",
    "            classifier_feedforward = Maxout.from_params(classifier_feedforward)\n",
    "        use_positional_encoding = params.pop(\"use_positional_encoding\", False)\n",
    "        bce_pos_weight = params.pop_int(\"bce_pos_weight\", 10)\n",
    "\n",
    "        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n",
    "        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n",
    "\n",
    "        return cls(vocab=vocab,\n",
    "                   text_field_embedder=text_field_embedder,\n",
    "                   abstract_text_encoder=abstract_text_encoder,\n",
    "                   attention_encoder=attention_encoder,\n",
    "                   classifier_feedforward=classifier_feedforward,\n",
    "                   bce_pos_weight=bce_pos_weight,\n",
    "                   use_positional_encoding=use_positional_encoding,\n",
    "                   initializer=initializer,\n",
    "                   regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]01/10/2019 17:20:26 - INFO - __main__ -   Reading instances from lines in file at: /misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\n",
      "955it [00:01, 688.49it/s]\n",
      "0it [00:00, ?it/s]01/10/2019 17:20:27 - INFO - __main__ -   Reading instances from lines in file at: /misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\n",
      "955it [00:00, 1017.31it/s]\n",
      "01/10/2019 17:20:28 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 1910/1910 [00:00<00:00, 4282.46it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = EtdAbstractReader()\n",
    "train_dataset = reader.read(cached_path(\n",
    "    \"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\"))\n",
    "validation_dataset = reader.read(cached_path(\n",
    "    \"/misc/projdata3/info_fil/finance/simon_test/ETD_cataloguing/allennlp-test/data/etd_debug.json\"))\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
